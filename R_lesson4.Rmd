---
title: "Introduction to R - part 4"
subtitle: "ðŸ§‰"
author: "Tiago Nardi"
institute: "University of Pavia"
output:
  xaringan::moon_reader:
    css: [extra.css, xaringan-themer.css]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    includes:
      after_body: insert-logo.html
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

```
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(gt)
library(DiagrammeR)
  #style_mono_accent(
  style_duo(
  primary_color = "#f2DB86",
  #base_color = "#B2284B",
  secondary_color = "#32577F",

)

# style_duo_accent(primary_color = "#002fa7", secondary_color = "#C5A900")

```
class: middle
## Statistical analysis - the basics
Statistical analysis is the science of collecting, exploring and presenting large amounts of data to discover underlying patterns and trends

Statistical analysis can be broken down into five discrete steps, as follows:

1. Describe the nature of the data to be analyzed
2. Explore the relation of the data to the underlying population
3. Create a model to summarize understanding of how the data relates to the underlying
population
4. Prove (or disprove) the validity of the model
5. Employ predective analyses to run scenarios that will help guide future actions

---
## Statistical analysis - the basics
Variable: characteristic detected on each statistical unit, which can take different values in the different statistical units

Observation: value assumed by a variable in a given statistical unit
```{r, include=FALSE}
df_log <- data.frame(Statistical_unit=c("Individual1","Individual2","Individual3"),
                Variable=c("a","b","c")
)
gt_tbl <- gt(df_log)
```

```{r, echo=FALSE}
gt_tbl
```
---
## Variables classification
```{r, eval=TRUE, echo=FALSE}
grViz(diagram = "digraph flowchart {
  node [fontname = arial, shape = box, color=Steelblue]
  tab1 [label = '@@1']
  tab2 [label = '@@2']
  tab3 [label = '@@3']
  tab4 [label = '@@4']
  tab5 [label = '@@5']
  tab6 [label = '@@6']
  tab7 [label = '@@7']
  
  tab1 -> tab2;
  tab1 -> tab3;
  tab2 -> tab4;
  tab2 -> tab5;
  tab3 -> tab6;
  tab3 -> tab7
}
  
  [1]: 'Variables'
  [2]: 'Quantitative Variables'    
  [3]: 'Qualitative variables'
  [4]: 'Discrete'
  [5]: 'Continuous'
  [6]: 'Nominal'
  [7]: 'Ordinal'
  ")
```
---
## Variables classification
### Quantitative variables
  expressed by numbers
- Discrete: can only be particular values within a certain interval (often derived from counts). They are natural numbers
  - Number of colonies per plate
  - Number of chromosomes of a species
- Continuous: can be any value within a given range (they come from measurements). They
are real numbers
  - Weight
  - Height

---
## Variables classification
### Qualitative variables
- #### Nominal are characterized by different modes that can not be ordered:
  - Sex: male / female
  - Survival: alive / dead
  - Blood groups: A, B, AB, 0
- #### Ordinal are characterized by different modes that can be ordered:
  - Obesity levels: overweight, obesity I, obesity II, obesity III
  - Intensity of reaction to an antigen: zero, medium, high
  - Educational qualification: compulsory school, bachelor, master, PhD
---
## Analysis flowchart
```{r, eval=TRUE, echo=FALSE}
grViz(diagram = "digraph flowchart {
  node [fontname = arial, shape = box, color=Steelblue]
  tab1 [label = '@@1']
  tab2 [label = '@@2', color=Red]
  tab3 [label = '@@3']
  tab4 [label = '@@4']
  tab5 [label = '@@5']
  tab6 [label = '@@6']
  tab7 [label = '@@7']
  
  tab1 -> tab2;
  tab2 -> tab3;
  tab3 -> tab4;
  tab4 -> tab5;
  tab3 -> tab6;
  tab6 -> tab7
}
  
  [1]: 'Quantitative Variables'
  [2]: 'Check if the variable has a normal distribution'    
  [3]: 'Shapiro-Wilk Normality Test'
  [4]: 'p-value < 0.05'
  [5]: 'Wilcoxon Test'
  [6]: 'p-value >= 0.05'
  [7]: 'Studentâ€™s t Test'
  ")
```
---
class: middle
## Statistical analysis - the basics
What is a normal distribution?

The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side

The area under the normal distribution curve represents probability and the total area under the curve sums to one.

Most of the continuous data values in a normal distribution tend to cluster around the
mean, and the further a value is from the mean, the less likely it is to occur
---
## Normal distribution

```{r normal, eval=TRUE, echo = FALSE,, dev.args = list(bg = "transparent")}
normal_distr <- rnorm (n = 50000,
                    mean = 0,
                    sd = 1)
#hist(ab_normal,breaks=100, col="#32577F")
myhist <- hist(normal_distr,breaks=100,col="#32577F")
multiplier <- myhist$counts / myhist$density
mydensity <- density(normal_distr)
mydensity$y <- mydensity$y * multiplier[1]

lines(mydensity,col="red",lwd = 4)

```
---
## Testing the normality
.pull-left[
To the test the normality we use the Shapiro-Wilk test

```{r, eval=FALSE}
shapiro.test(numeric_vector)
```
- The null hypothesis for this test is that the data are normally distributed

- The alternative hypothesis for this test is that the data are not normally distributed
]
.pull-right[![](img/abby.gif)]
---
class: middle
## Testing the normality
- If the __p-value__ is less than __0.05__ (5%), we rejected the null hypothesis and we assume the variable does not have a __normal distribution__

- If the __p-value__ is more than  __0.05__ (5%), we assume the null hypothesis is true   and we assume that the variable has a __normal distribution__

---
class: inverse, middle
## Important Reminder
.pull-left[
### The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct]
.pull-right[![](img/cards.gif)]
---
class: inverse, middle, center
## Other Reminder
### The 0.05 (1/20) is a commonly used threshold, often considered adeguate. It's used a standard value, but there is no inherent reasons to prefer this specific value

---
class: inverse, middle, center, hide-logo
## In RA Fisher own words
.pull-left[
#### "If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty or one in a hundred. Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fails to reach this level. A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance"
Fisher, R. A. 1926. The arrangement of field experiments. Journal of the
Ministry of Agriculture. 33, pp. 503-515]

.pull-right[![](img/fisher.jpg)]
---
class: middle
## Have a try
Load the usual tableand see if the *Contigs* variable has a normal distribution 
```{r eval=FALSE}
db <- read.csv("patric_redux.csv")
shapiro.test(vector_name)
```
---
## Shapiro test
```{r eval=TRUE}
db <- read.csv("patric_redux.csv")
shapiro.test(db$Contigs)
```
__p-value__ lesser than 0.05

We assume that the distribution is not normal

This information is used to decide which test to use for hypothesis testing
---
## Hypothesis testing

We test if the __mean__ of two (or more) values are __significantly different__

- The null hypothesis for this test is that the two mean values are not statistically
different
- The alternative hypothesis for this test is that the two mean values are statistically
different


- If the __p-value__ is less than __0.05__ (5%), we rejected the null hypothesis and we assume the two mean values are __significantly different__

- If the __p-value__ is more than  __0.05__ (5%), we assume the null hypothesis is true   and we assume that the two mean values are not __significantly different__
---
## Hypothesis testing
When the sample is normally distributed we use the T test

When the sample is not normally distributed we use the Wilcoxon test

Wilcoxon test measures medians, not means 

``` {r, eval= FALSE}
wilcox.test(first_numeric_vector, second_numeric_vector)

t.test(first_numeric_vector, second_numeric_vector)
```
![](img/gosset.jpg)
---
## Exercise
.pull-left[
We want to check if the samples with Source from *Hobbit* have a different mean of *Contigs* compared to sample with a Source from *Human*

- Use the *patric_redux.csv* dataset and use subset to have the *Contigs* for the two categories
- Check if the *Contigs* variable has a normal distribution and test the hypothesis with the correct test]
.pull-right[![](img/cowboy-bebop-cowboy.gif)]
```{r eval=FALSE}
db <- read.csv("patric_redux.csv")
shapiro.test(numeric_vector)
wilcox.test(first_numeric_vector, second_numeric_vector)
t.test(first_numeric_vector, second_numeric_vector)

```
---
## Results
```{r}
db <- read.csv("patric_redux.csv")
hobbit <- subset(db,db$Source=="Hobbit")
human <- subset(db,db$Source=="Human")
shapiro.test(hobbit$Contigs)
shapiro.test(human$Contigs)
```

---
## Results
As one of them is not normal we use the Wilcoxon
```{r}
wilcox.test(hobbit$Contigs,human$Contigs)
```

---
## Chi squared test
Test whether there is an association between categorical variables
```{r, eval=TRUE, echo=FALSE}
grViz(diagram = "digraph flowchart {
  node [fontname = arial, shape = box, color=Steelblue]
  tab1 [label = '@@1']
  tab2 [label = '@@2']
  tab3 [label = '@@3',color=red]
  tab4 [label = '@@4',color=red]

  tab1 -> tab2;
  tab2 -> tab3;
  tab2 -> tab4
}
  
  [1]: 'Qualitative Variables'
  [2]: 'Chi-Square Test'    
  [3]: 'p-value < 0.05'
  [4]: 'p-value >= 0.05'
  ")
```
---
## Chi-square test

Chi-square test requires a contingency table

Contingency table (two-way table): data is classified with two categorical variables.
In the rows we have the categories for one variable, and in the columns the categories for the other variable.

Each variable must have two or more categories. 

Each cell reflects the total count of cases for a specific pair of categories
```{r, eval=TRUE}
db_red <- subset(db,(db$Isolation_location=="The Shire" |
                       db$Isolation_location=="Gondor"))
cont_db <- table(db_red$Isolation_location, db_red$Source)
cont_db
```

---
## Chi-squared test
- The null hypothesis for this test is that the categorical variables are independent (no correlation)
- The alternative hypothesis for this test is that the categorical variables are correlated

```{r, eval=TRUE}
chisq.test(cont_db)
```
Warning message as the dataset is limited
---
## Exercise
.pull-left[
Using the subsetted database (only from *The Shire* and *Gondor*), we want to check if there is an association between the *Species* of the bacteria and the *Source* of the isolate

- Prepare a contingency table
- Check the hypothesis Chi-squared]
.pull-right[![](img/cowboy-bebop-cowboy.gif)]
```{r, eval=FALSE}
subsetted_table <- subset(table,logical_condition)
contigency_table <- table(var1,var2)
chisq.test(contigency_table)
```
---
## Results
```{r, eval=TRUE}
db_red <- subset(db,(db$Isolation_location=="The Shire" |
                       db$Isolation_location=="Gondor"))
cont_db <- table(db_red$Species, db_red$Source)
cont_db
chisq.test(cont_db)
```
So there is a significant association between these two categories
